{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "from datetime import datetime\n",
    "import progressbar\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize, regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing of post\n",
    "def parse_post(post):\n",
    "    output_post = {}\n",
    "    output_post['id'] = post['id']\n",
    "    output_post['url'] = post['url']\n",
    "    output_post['dateRFC'] = post['dateRFC']\n",
    "    output_post['date'] = post['date']\n",
    "    output_post['title'] = post['title']\n",
    "    output_post['intro'] = post['intro']\n",
    "    text = ''\n",
    "    \n",
    "    # Собираем текст в одно целое\n",
    "    for block in post['blocks']:\n",
    "        if block['type'] == 'text' or block['type'] == 'quote':\n",
    "            text += block['data']['text'] + ' '\n",
    "        if block['type'] == 'list':\n",
    "            text += ' '.join(block['data']['items']) + ' '\n",
    "        if block['type'] == 'rawhtml':\n",
    "            text += cleanhtml(block['data']['raw']) + ' '\n",
    "            \n",
    "    # Удаляем штуки которые мешают читать текст\n",
    "    text = text.replace('[','', -1)\n",
    "    text = text.replace(']','', -1)\n",
    "    text = text.replace(\"\\r\", \" \")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    text = text.replace(\"&nbsp\", \" \")\n",
    "    text = text.replace(\"    \", \" \")\n",
    "    output_post['text'] = re.sub(r'\\(http\\S+\\)', '', text, flags=re.MULTILINE)\n",
    "    return output_post\n",
    "\n",
    "def save_post_to_json(path, post):\n",
    "    name = 'post' + str(post['id']) + '.json'\n",
    "    with open(join(path, name), 'w', encoding='utf-8') as output:\n",
    "        json.dump(post, output, indent=4, ensure_ascii=False)\n",
    "        \n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "bar = progressbar.ProgressBar(maxval=41000).start()\n",
    "# 3290 - 59400 offset\n",
    "for offset in range(4700, 41000, 50):\n",
    "    req_url = 'https://api.tjournal.ru/v1.8/timeline/index/recent?count=50&offset=' + str(offset)\n",
    "    request = requests.get(req_url)\n",
    "    data = json.loads(request.text)\n",
    "    bar.update(offset)\n",
    "    for item in data['result']:\n",
    "        post = parse_post(item)\n",
    "        date = datetime.fromtimestamp(post['date'])\n",
    "        if date.year < 2020 and date.year > 2013 and len(post['text']) > 400:\n",
    "            save_post_to_json(join('D:\\\\some stuff\\\\tj', str(date.year)), post)\n",
    "    \n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_names = ['2014', '2015', '2016', '2017', '2018', '2019']\n",
    "\n",
    "path_glob = 'D:\\\\some stuff\\\\tj'\n",
    "\n",
    "rows = []\n",
    "for folder in folder_names:\n",
    "    path = join(path_glob, folder)\n",
    "    posts_names = listdir(path)\n",
    "    for post in posts_names:\n",
    "        with open(join(path, post), encoding='utf-8') as f:\n",
    "            r = json.load(f)\n",
    "            rows.append(r)\n",
    "            \n",
    "df = pd.DataFrame.from_dict(rows, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26025, 7)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>dateRFC</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48962</td>\n",
       "      <td>https://tjournal.ru/internet/48962-meme-2013</td>\n",
       "      <td>Wed, 01 Jan 2014 12:24:26 +0400</td>\n",
       "      <td>1388564666</td>\n",
       "      <td>Мемы, которые мы заслужили</td>\n",
       "      <td>Гарлем шейк, тверкинг, кошка Путина, озадаченн...</td>\n",
       "      <td>Гарлем шейк, тверкинг, кошка Путина, озадаченн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49173</td>\n",
       "      <td>https://tjournal.ru/internet/49173-kernes</td>\n",
       "      <td>Wed, 19 Feb 2014 01:00:06 +0400</td>\n",
       "      <td>1392757206</td>\n",
       "      <td>Типичный харьковский мэр</td>\n",
       "      <td>Мэра Харькова Геннадия Кернеса сравнивают с Ка...</td>\n",
       "      <td>Мэра Харькова Геннадия Кернеса сравнивают с Ка...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49237</td>\n",
       "      <td>https://tjournal.ru/tech/49237-2013-in-gadgets</td>\n",
       "      <td>Thu, 02 Jan 2014 17:12:51 +0400</td>\n",
       "      <td>1388668371</td>\n",
       "      <td>Рак, лебедь и айфон</td>\n",
       "      <td>Главные гаджеты 2013 года</td>\n",
       "      <td>Главные гаджеты 2013 года В последние годы люб...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49242</td>\n",
       "      <td>https://tjournal.ru/tech/49242-iphone-nsa</td>\n",
       "      <td>Wed, 01 Jan 2014 17:09:07 +0400</td>\n",
       "      <td>1388581747</td>\n",
       "      <td>АНБ хвасталось полным доступом к iPhone</td>\n",
       "      <td>В документах Агентства национальной безопаснос...</td>\n",
       "      <td>В документах Агентства национальной безопаснос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49245</td>\n",
       "      <td>https://tjournal.ru/tech/49245-sea-skype</td>\n",
       "      <td>Thu, 02 Jan 2014 02:45:56 +0400</td>\n",
       "      <td>1388616356</td>\n",
       "      <td>Электронная армия Сирии от имени Skype призвал...</td>\n",
       "      <td>Участники кибергруппировки, называющей себя «э...</td>\n",
       "      <td>Участники кибергруппировки, называющей себя «э...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                             url  \\\n",
       "0  48962    https://tjournal.ru/internet/48962-meme-2013   \n",
       "1  49173       https://tjournal.ru/internet/49173-kernes   \n",
       "2  49237  https://tjournal.ru/tech/49237-2013-in-gadgets   \n",
       "3  49242       https://tjournal.ru/tech/49242-iphone-nsa   \n",
       "4  49245        https://tjournal.ru/tech/49245-sea-skype   \n",
       "\n",
       "                           dateRFC        date  \\\n",
       "0  Wed, 01 Jan 2014 12:24:26 +0400  1388564666   \n",
       "1  Wed, 19 Feb 2014 01:00:06 +0400  1392757206   \n",
       "2  Thu, 02 Jan 2014 17:12:51 +0400  1388668371   \n",
       "3  Wed, 01 Jan 2014 17:09:07 +0400  1388581747   \n",
       "4  Thu, 02 Jan 2014 02:45:56 +0400  1388616356   \n",
       "\n",
       "                                               title  \\\n",
       "0                         Мемы, которые мы заслужили   \n",
       "1                           Типичный харьковский мэр   \n",
       "2                                Рак, лебедь и айфон   \n",
       "3            АНБ хвасталось полным доступом к iPhone   \n",
       "4  Электронная армия Сирии от имени Skype призвал...   \n",
       "\n",
       "                                               intro  \\\n",
       "0  Гарлем шейк, тверкинг, кошка Путина, озадаченн...   \n",
       "1  Мэра Харькова Геннадия Кернеса сравнивают с Ка...   \n",
       "2                          Главные гаджеты 2013 года   \n",
       "3  В документах Агентства национальной безопаснос...   \n",
       "4  Участники кибергруппировки, называющей себя «э...   \n",
       "\n",
       "                                                text  \n",
       "0  Гарлем шейк, тверкинг, кошка Путина, озадаченн...  \n",
       "1  Мэра Харькова Геннадия Кернеса сравнивают с Ка...  \n",
       "2  Главные гаджеты 2013 года В последние годы люб...  \n",
       "3  В документах Агентства национальной безопаснос...  \n",
       "4  Участники кибергруппировки, называющей себя «э...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles_tj\\\\big_df.pickle', 'wb') as output:\n",
    "    pickle.dump(df, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df = \"pickles_tj\\\\big_df.pickle\"\n",
    "\n",
    "with open(path_df, 'rb') as data:\n",
    "    df = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_stopwords_ru = \"https://raw.githubusercontent.com/stopwords-iso/stopwords-ru/master/stopwords-ru.txt\"\n",
    "\n",
    "\n",
    "def get_text(url, encoding='utf-8', to_lower=True):\n",
    "    url = str(url)\n",
    "    if url.startswith('http'):\n",
    "        r = requests.get(url)\n",
    "        if not r.ok:\n",
    "            r.raise_for_status()\n",
    "        return r.text.lower() if to_lower else r.text\n",
    "    elif os.path.exists(url):\n",
    "        with open(url, encoding=encoding) as f:\n",
    "            return f.read().lower() if to_lower else f.read()\n",
    "    else:\n",
    "        raise Exception('parameter [url] can be either URL or a filename')\n",
    "\n",
    "\n",
    "def normalize_tokens(tokens):\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    return [morph.parse(tok)[0].normal_form for tok in tokens]\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens, stopwords=None, min_length=4):\n",
    "    if not stopwords:\n",
    "        return tokens\n",
    "    stopwords = set(stopwords)\n",
    "    tokens = [tok\n",
    "              for tok in tokens\n",
    "              if tok not in stopwords and len(tok) >= min_length]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize_n_lemmatize(\n",
    "    text, stopwords=None, normalize=True, \n",
    "    regexp=r'(?u)\\b\\w{4,}\\b'):\n",
    "    words = [w for sent in sent_tokenize(text)\n",
    "             for w in regexp_tokenize(sent, regexp)]\n",
    "    if normalize:\n",
    "        words = normalize_tokens(words)\n",
    "    if stopwords:\n",
    "        words = remove_stopwords(words, stopwords)\n",
    "    return words\n",
    "\n",
    "stopwords_ru = get_text(url_stopwords_ru).splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_df(df):\n",
    "    nrows = len(df)\n",
    "    bar = progressbar.ProgressBar(maxval=nrows).start()\n",
    "    text_parsed_list = []\n",
    "    for row in range(0, nrows):\n",
    "        bar.update(row)\n",
    "        text = df.loc[row]['title'] + ' ' + df.loc[row]['text']\n",
    "        text = text.replace('_', ' ')\n",
    "        text_parsed_list.append(' '.join(tokenize_n_lemmatize(text)))\n",
    "    df['text_parsed'] = text_parsed_list\n",
    "    bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "proc_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>dateRFC</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>text</th>\n",
       "      <th>text_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48962</td>\n",
       "      <td>https://tjournal.ru/internet/48962-meme-2013</td>\n",
       "      <td>Wed, 01 Jan 2014 12:24:26 +0400</td>\n",
       "      <td>1388564666</td>\n",
       "      <td>Мемы, которые мы заслужили</td>\n",
       "      <td>Гарлем шейк, тверкинг, кошка Путина, озадаченн...</td>\n",
       "      <td>Гарлем шейк, тверкинг, кошка Путина, озадаченн...</td>\n",
       "      <td>мема который заслужить гарлем шейк тверкинг ко...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49173</td>\n",
       "      <td>https://tjournal.ru/internet/49173-kernes</td>\n",
       "      <td>Wed, 19 Feb 2014 01:00:06 +0400</td>\n",
       "      <td>1392757206</td>\n",
       "      <td>Типичный харьковский мэр</td>\n",
       "      <td>Мэра Харькова Геннадия Кернеса сравнивают с Ка...</td>\n",
       "      <td>Мэра Харькова Геннадия Кернеса сравнивают с Ка...</td>\n",
       "      <td>типичный харьковский мэр харьков геннадий керн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49237</td>\n",
       "      <td>https://tjournal.ru/tech/49237-2013-in-gadgets</td>\n",
       "      <td>Thu, 02 Jan 2014 17:12:51 +0400</td>\n",
       "      <td>1388668371</td>\n",
       "      <td>Рак, лебедь и айфон</td>\n",
       "      <td>Главные гаджеты 2013 года</td>\n",
       "      <td>Главные гаджеты 2013 года В последние годы люб...</td>\n",
       "      <td>лебедь айфон главное гаджет 2013 год последний...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49242</td>\n",
       "      <td>https://tjournal.ru/tech/49242-iphone-nsa</td>\n",
       "      <td>Wed, 01 Jan 2014 17:09:07 +0400</td>\n",
       "      <td>1388581747</td>\n",
       "      <td>АНБ хвасталось полным доступом к iPhone</td>\n",
       "      <td>В документах Агентства национальной безопаснос...</td>\n",
       "      <td>В документах Агентства национальной безопаснос...</td>\n",
       "      <td>хвастаться полный доступ iphone документ агент...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49245</td>\n",
       "      <td>https://tjournal.ru/tech/49245-sea-skype</td>\n",
       "      <td>Thu, 02 Jan 2014 02:45:56 +0400</td>\n",
       "      <td>1388616356</td>\n",
       "      <td>Электронная армия Сирии от имени Skype призвал...</td>\n",
       "      <td>Участники кибергруппировки, называющей себя «э...</td>\n",
       "      <td>Участники кибергруппировки, называющей себя «э...</td>\n",
       "      <td>электронный армия сирия имя skype призвать бой...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                             url  \\\n",
       "0  48962    https://tjournal.ru/internet/48962-meme-2013   \n",
       "1  49173       https://tjournal.ru/internet/49173-kernes   \n",
       "2  49237  https://tjournal.ru/tech/49237-2013-in-gadgets   \n",
       "3  49242       https://tjournal.ru/tech/49242-iphone-nsa   \n",
       "4  49245        https://tjournal.ru/tech/49245-sea-skype   \n",
       "\n",
       "                           dateRFC        date  \\\n",
       "0  Wed, 01 Jan 2014 12:24:26 +0400  1388564666   \n",
       "1  Wed, 19 Feb 2014 01:00:06 +0400  1392757206   \n",
       "2  Thu, 02 Jan 2014 17:12:51 +0400  1388668371   \n",
       "3  Wed, 01 Jan 2014 17:09:07 +0400  1388581747   \n",
       "4  Thu, 02 Jan 2014 02:45:56 +0400  1388616356   \n",
       "\n",
       "                                               title  \\\n",
       "0                         Мемы, которые мы заслужили   \n",
       "1                           Типичный харьковский мэр   \n",
       "2                                Рак, лебедь и айфон   \n",
       "3            АНБ хвасталось полным доступом к iPhone   \n",
       "4  Электронная армия Сирии от имени Skype призвал...   \n",
       "\n",
       "                                               intro  \\\n",
       "0  Гарлем шейк, тверкинг, кошка Путина, озадаченн...   \n",
       "1  Мэра Харькова Геннадия Кернеса сравнивают с Ка...   \n",
       "2                          Главные гаджеты 2013 года   \n",
       "3  В документах Агентства национальной безопаснос...   \n",
       "4  Участники кибергруппировки, называющей себя «э...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Гарлем шейк, тверкинг, кошка Путина, озадаченн...   \n",
       "1  Мэра Харькова Геннадия Кернеса сравнивают с Ка...   \n",
       "2  Главные гаджеты 2013 года В последние годы люб...   \n",
       "3  В документах Агентства национальной безопаснос...   \n",
       "4  Участники кибергруппировки, называющей себя «э...   \n",
       "\n",
       "                                         text_parsed  \n",
       "0  мема который заслужить гарлем шейк тверкинг ко...  \n",
       "1  типичный харьковский мэр харьков геннадий керн...  \n",
       "2  лебедь айфон главное гаджет 2013 год последний...  \n",
       "3  хвастаться полный доступ iphone документ агент...  \n",
       "4  электронный армия сирия имя skype призвать бой...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles_tj\\\\parsed_big_df.pickle', 'wb') as output:\n",
    "    pickle.dump(df, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles_tj/parsed_big_df.pickle', 'rb') as data:\n",
    "    df = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/best_rfc_fin.pickle', 'rb') as data:\n",
    "    rfc_fin = pickle.load(data)\n",
    "    \n",
    "with open('models/best_rfc_inv.pickle', 'rb') as data:\n",
    "    rfc_inv = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Pickles/tfidf.pickle', 'rb') as data:\n",
    "    tfidf = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter election\n",
    "ngram_range = (1, 1)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 5000\n",
    "\n",
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "\n",
    "features = tfidf.fit_transform(df['text_parsed']).toarray()\n",
    "\n",
    "# TF-IDF object\n",
    "with open('pickles/tfidf.pickle', 'wb') as output:\n",
    "    pickle.dump(tfidf, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Pickles/tfidf.pickle', 'rb') as data:\n",
    "    tfidf = pickle.load(data)\n",
    "features = tfidf.transform(df['text_parsed']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_pred = rfc_fin.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_pred = rfc_inv.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=int64), array([25752,   273], dtype=int64))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fin_pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0], dtype=int64), array([26025], dtype=int64))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(inv_pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = [0] * 72\n",
    "inv = [0] * 72\n",
    "total = [0] * 72\n",
    "for i in range(len(df)):\n",
    "    date = datetime.fromtimestamp(df.loc[i]['date'])\n",
    "    j = (date.year - 2014) * 12 + date.month - 1 \n",
    "    total[j] += 1\n",
    "    if fin_pred[i] == 1:\n",
    "        fin[j] += 1\n",
    "    if inv_pred[i] == 1:\n",
    "        inv[j] += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Total' : total, 'Fintech' : fin} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv('tj.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_res = [0] * 72\n",
    "fin_res = [0] * 72\n",
    "inv_res = [0] * 72\n",
    "words_tf_fin = ['финтех', 'криптовалюта', 'блокчейн', 'краудфандинг']\n",
    "words_tf_fin_p = [('инвестиция','стартап'),('инвестиция', 'финтех'),('онлайн', 'банк'), ('финансовый', 'рынок')]\n",
    "words_tf_inv = [('венчурный', 'капитал'), ('венчурный', 'финансирование')]\n",
    "for i in range(len(df)):\n",
    "    date = datetime.fromtimestamp(df.loc[i]['date'])\n",
    "    j = (date.year - 2014) * 12 + date.month - 1 \n",
    "    tot_res[j] += 1\n",
    "    f = False\n",
    "    for w in words_tf_fin:\n",
    "        if df.loc[i]['text_parsed'].find(w) != -1:\n",
    "            f = True\n",
    "            fin_res[j] += 1\n",
    "            break\n",
    "    if f == False:    \n",
    "        for w1, w2 in words_tf_fin_p:\n",
    "            if df.loc[i]['text_parsed'].find(w1) != -1 and df.loc[i]['text_parsed'].find(w2) != -1:\n",
    "                fin_res[j] += 1\n",
    "                break\n",
    "    for w1, w2 in words_tf_inv:\n",
    "            if df.loc[i]['text_parsed'].find(w1) != -1 and df.loc[i]['text_parsed'].find(w2) != -1:\n",
    "                inv_res[j] += 1\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Total' : tot_res, 'Fintech' : fin_res, 'Investment': inv_res} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv('tj_key_words.csv', sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
